% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath,epsfig}
\usepackage{amsthm, amssymb}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\usepackage{ textcomp }

%%% END Article customizations


%%% The "real" document content comes below...

\title{LING773 - Negotiations Project Report}
\author{Peng Ye, Youngil Kim, Olivia Buzek}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle


\section{Introduction}
brief introduction
Who is in this group. Optionally, a rough breakdown of people's roles in the project,
\section{Preprocessing}

\section{Feature extraction}
In this section, we will introduce three types of features that are extracted for predicting joint-profit.
\subsection{Meta information}

\subsection{Code-based N-Gram feature}
In our data set, every thought unit is carefully coded by social scientist. The code of a thought unit is like a summary about this thought unit. It helps us to understand the functionality of a thought unit. Some code indicates positive interaction between speakers, for example, 'RP' means positive reaction; some code indicates negative interaction, for example, 'RN' means negative reaction. Intuitively, negative or positive interaction should have high correlation with joint profit, thus thought unit code is informative feature for predicting joint profit. The second type of feature we use is though unit code based feature. Consider a dyad as a sequence of code, we then extract code-based unigram and bigram as our featurs. Specifically, for each dyad we extract the following features:
\begin{itemize}
\item Unigram based features:
\begin{itemize}
\item Overall features: (Overall features are extracted from the entire dyad.)

      number of total thought units, number of thought units of each code, percentage of thought units of each code
\item Wine features: (Wine features are extracted from wine part of the dyad)

      number of total thought units (in wine part), number of thought units of each code, percentage of thought units of each code

\item Grocery features: (Grocery features are extracted from grocery part of the dyad)

number of total thought units (in grocery part), number of thought units of each code, percentage of thought units of each annotation

\item Relative feature: Relative feature indicates the difference between wine part and grocery part of the dyad.

J-S divergence between the unigram distribution over wine dialog and unigram distribution over grocery dialog
\end{itemize}
\item Bigram based features:
\begin{itemize}
\item Overall features: (Overall features are extracted from the entire dyad.)

      number of turns (change of speakers) in the dyad, number of code-based bigram, percentage of code-based bigram
\item Wine features: (Wine features are extracted from wine part of the dyad)

      number of code-based bigram, percentage of code-based bigram
\item Grocery features: (Grocery features are extracted from grocery part of the dyad)

      number of code-based bigram, percentage of code-based bigram
\item Turning point features: A turning point in a dyad is where the speaker changes. Features extracted at turning points indicate how one speaker respond the other speaker.

      number of code-based bigram, percentage of code-based bigram

\item Relative feature:

      J-S divergence between the bigram distribution over wine dialog and bigram distribution over grocery dialog
\end{itemize}
\end{itemize}

In our corpus, we have 38 code-based unigram and 660 code-based bigram. We extract a total of 5515 code-based features from each dyad.

It is worth noting that features introduced above are redundant, for example the percentage of a unigram and the number of times a unigram occurred are correlated with each other and are only different by a constant factor, in the initial feature extraction part, we are not going to eliminate those redundant features, instead, we would like to collect as many features as possible and perform feature selection on these features to choose the most informative features. Feature selection would be illustrated in detail in next section.

\subsection{Word-based N-Gram feature}
We extract word-based n-grams through following process.
\begin{itemize}
\item Preprocessing with raw data - creating \textit{fields\_speaker.txt}: \newline
We analyzed \textit{merged\_turns.csv} and \textit{metadata.csv} for creating \textit{fields\_speaker.txt} file. Because some dialogues do not have \textit{joint profit} in \textit{metadata.csv} file, we should rule out those dialogues even though they exist in \textit{merged\_turns.csv} file. In addition, we rearranaged sequence of fields for better looking; the dyid is positioned as the first item.

\item Tokeninzing with whole dialog files: \newline
The raw data contains many special characters and not yet tokenized for creating n-grams. Basically, we used Treebank tokenization and this can be found in \textit{http://www.cis.upenn.edu/$\sim$treebank/tokenization.html}. Following list shows a part of the tokenization rules.
\begin{itemize}
\item most punctuation is split from adjoining words.
\item verb contractions and the Anglo-Saxon genitive of nouns are split into their component morphemes, and each morpheme is tagged separately.
\begin{itemize}
\item Examples
\begin{itemize}
\item children's \textrightarrow children 's
\item I'm \textrightarrow I 'm
\end{itemize}
\end{itemize}
This tokenization allows us to analyze each component separately.\newline
In addition, we need to correct some words to see more precise result. For example, some speakers omit \textit{question mark} or \textit{apostrophe}, and sometimes there are several special characters which is not interpretable in plain text. You can see the detailed conversion in \textit{tokenize} function in \textit{create\_ngram.py}.
\end{itemize}
\item Count n-grams and Create n-gram ID:\newline
After tokenizing, we count number of n-grams to see the characteristics of each dialogue. For counting ngrams, we used \textit{count.pl} in Ted Pedersen Ngram Statics Package with general stop words because stop words can hide real characteristics of n-grams. We seperately count n-grams for Wine, Grocery, and all dialogues for seeing the characteristics of each group of speakers. Also, we count n-grams per each dyad too. \newline
In addition, we created n-gram id files to express ngrams. For example, \textit{grand opening date} which is one of trigram is expressed as \textit{3-0001}. By using this ngram id, we can save memory and storage for analyzing whole data.
\item Make n-gram feature file:\newline
By using counted n-grams, we created n-gram feature file which can be converted into an ARFF format, input format of Weka. Each row in the file contains n-gram information of each dyad. The first column shows the \textit{dyID}, and the following colums show ngram id and the number of ngrams in the dialogues. Following example shows a part of the feature file.
\begin{itemize}
\item Example
\begin{verbatim}
02,1-0002:30,1-0001:23,1-0040:18,1-0009:16,1-0012:15 ... ...
03,1-0006:30,1-0001:29,1-0048:23,1-0026:21,1-0025:20 ... ...
\end{verbatim}
\end{itemize}
We made three n-gram feature files for each category, Wine, Grocery, and all dialogues.
\end{itemize}

In our experiment, we actually used only the n-gram extracted from the entire dyad only. We have a total of 5754 word-based ngram features.

\section{Regression}
\subsection{Feature Selection}
\label{sec:fv_selection}
Feature subset selection is a process of identifying the most informative features and removing irrelevant and redundant features. In our experiment, we extracted a large number of features, however, the total number of instances in only 61, without feature selection, the training would be very slow. In Weka, we use 'BestFirst' as search method and use 'CfsSubsetEval' as attribute evaluator for finding the most informative features. As is explained in Weka, 'CfsSubsetEval' evaluates ``the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them'' and 'BestFirst' searches ``the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done.'' In our experiment, best first search start with the full set of attributes and search backward. The top ten best word based ngram feature is shown in Table.\ref{tab:selected_ngram} and the top ten best code based ngram feature is shown in Table. In this table, ``percentage-of-(MIN-QR)-turns'' means the percentage of the bigram of MIN-QR extracted from turning points of a dyad.
\begin{table}
  \centering
  \caption{THE TOP TEN MOST INFORMATIVE WORD-BASED AND CODE-BASED NGRAM FEATURE}
  \begin{tabular}{|c|c|}
     \hline
CODE-BASED NGRAM FEATURE & WORD-BASED NGRAM \\
  \hline
percentage-of-(MIN-QR)-turns & hours \\

percentage-of-(SF)-overall & special\\

percentage-of-(SF)-grocery & great\\

number-of-(IR)-overall & 30k\\

percentage-of-(SBR-SF)-overall & agreeable\\

percentage-of-(OM-QM)-wine & 630\\

percentage-of-(MIN-IR)-wine & 1030pm\\

percentage-of-(IP-SF)-grocery & luxurious\\

percentage-of-(IP-IDN)-turn & people work\\

percentage-of-(IDN-SBR)-turns & crust grocery\\
  \hline
  \end{tabular}\label{tab:selected_ngram}
\end{table}

We extract the top 20 most informative code-based features and word-based feature, together with features extracted from meta information as the final feature. Specifically, we have a total of 49 features for each of the 61 dyads. It should be noticed that feature selection performed on all the available data may lead to overfitting problem, however, we would like to first perform feature selection on all data to see which features are the best for our current dataset. In the following experimental part, we will split dataset into training and testing part and feature selection would be only performed on training data to make sure we won't suffer overfitting problem.

\subsection{Regression Algorithms}
We used three different types of regression methods provided in Weka for predicting joint profit.
\begin{itemize}
\item weka.classifiers.functions.SMOreg \cite{SMO,SVM}: support vector machine for regression, and we use RBF kernel. 
\item weka.classifiers.functions.LinearRegression: using linear regression for prediction.
\item weka.classifiers.meta.Bagging + weka.classifiers.trees.REPTree \cite{bagging}: bagging predictor based on REPTree which builds a regression tree using information gain.
\end{itemize}

\subsection{Evaluation Metrics}
The following metrics are used to evaluate the proposed method.

1. Correlation coefficient (CC)

2. Mean absolute error (MAE)

3. Root mean squared error (RMSE)

4. Relative absolute error (RAE)

5. Root relative squared error (RRSE)


\section{Experiment results}
In this section, we will show the experimental results using the features (49-by-1 vectors) obtained through the feature selection process and the three different regression methods. This result is compared to a baseline experiment, where code-based unigram features and meta information are directly used to form a 241-by-1 feature vector (without further feature selection process) for each dyad. Our experimental results show that using those selected features, we can achieve better performance. We did two groups of experiments:
 
1) In the first experiment, the improved method uses feature obtained by running feature selection on the entire dataset and it is compared to baseline method described above. As is shown in Table \ref{tab:result1} and Table \ref{tab:result2}, the improved method consistently performs better than the baseline method when use SVM for regression. Compared to linear regression and bagging method, SVM regression provide more consistent and better performance.

\begin{table}
  \centering
  \caption{EXPERIMENT RESULTS WITH TEN FOLD CROSS-VALIDATION AND FEATURE SELECTION PERFORMED ON ALL DATA}
  \begin{tabular}{|c|c|c|c|c|c|c|}
     \hline
         &       & CC & MAE & RMSE & RAE & RRSE\\
     \hline
     SVM & Baseline & 0.72  & 57.93  & 74.01  & 64.90\%  & 70.44\%\\
         & Improved & \textbf{0.90}  & \textbf{43.03}  & \textbf{53.25}  & \textbf{48.20\%}  & \textbf{50.68\%}\\
     \hline
  Linear & Baseline &  0.71 & 64.32  & 86.42  & 72.05\% & 82.25\%\\
  Regression & Improved & 0.76  & 69.30  & 88.11  & 77.63\%  & 83.87\%  \\
  \hline
  REPTree & Baseline &  0.66 & 67.75  & 81.42  & 75.89\%  & 77.50\%\\
  Bagging & Improved &  0.69 & 64.16  & 77.01  & 71.87\%  & 73.49\%\\
  \hline
  \end{tabular}\label{tab:result1}
\end{table}

\begin{table}
  \centering
  \caption{EXPERIMENT RESULTS WITH FIVE FOLD CROSS-VALIDATION AND FEATURE SELECTION PERFORMED ON ALL DATA}
  \begin{tabular}{|c|c|c|c|c|c|c|}
     \hline
         &       & CC & MAE & RMSE & RAE & RRSE\\
     \hline
     SVM & Baseline & 0.68  & 66.33  & 83.57  & 74.15\%  & 79.63\%\\
         & Improved & \textbf{0.89}  & \textbf{44.31 } & \textbf{54.41}  & \textbf{49.53\% } & \textbf{51.85\%}\\
     \hline
  Linear & Baseline &  0.67 & 70.47  & 90.95  & 78.77\% & 86.67\%\\
  Regression & Improved & 0.53 & 82.82 & 105.08  & 92.58\%  & 100.13\%  \\
  \hline
  REPTree & Baseline &  0.51 & 73.06  & 89.84  & 81.67\%  & 85.61\%\\
  Bagging & Improved &  0.69 & 62.44  & 78.01  & 69.80\%  & 74.34\%\\
  \hline
  \end{tabular}\label{tab:result2}
\end{table}

2) In the second experiment, we first split our dataset into training and testing set, where in training set we have 46 dyads and in testing set, we have 15 dyads. We split the dataset in a way such that the distribution of the value of joint profit is ``uniform'' in both training and testing set.
We then perform the feature selection procedure mentioned in Section \ref{sec:fv_selection} on training set. The classifier training is also performed solely on training set. We then test the trained model on our testing set.

In this experiment, we only used SVM regression and we have the following result:

\begin{table}
  \centering
  \caption{EXPERIMENT RESULTS FEATURE SELECTION PERFORMED ON TRAINING DATA}
  \begin{tabular}{|c|c|c|c|c|c|c|}
     \hline
         &       & CC & MAE & RMSE & RAE & RRSE\\
     \hline
     SVM & Baseline (5-fold c-v)& 0.68  & 66.33  & 83.57  & 74.15\%  & 79.63\%\\
         & Improved & \textbf{0.76}  & \textbf{61.54 } & \textbf{71.58}  & \textbf{69.65\% } & \textbf{68.69\%}\\
  \hline
  \end{tabular}\label{tab:result3}
\end{table}

This result should be compared with the five fold cross-validation result of baseline method. As is shown in Tabel \ref{tab:result3}, although our improved approach still performs better than the baseline approach.
\bibliographystyle{IEEEbib}
\bibliography{refs}
\end{document}
