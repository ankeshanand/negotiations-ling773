% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margins=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath,epsfig}
\usepackage{amsthm, amssymb}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{LING773 - Negotiations Project Proposal}
\author{Peng Ye, Youngil Kim, Olivia Buzek}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle

\section{Introduction} % by Olivia

In this project, we will use feature engineering to better predict what the joint profit will be.  The first round will improve prediction using classification of dialogues into the broad categories LOW, MEDIUM, and HIGH.  The second round will attempt to predict the exact profits.  Our goal is to improve prediction without using any domain-specific features, such as the outcomes for the specific points addressed by the conversation partners in the Imai and Gelfand data.

\section{Preprocessing} % by Olivia

The data will be lowercased, punctuation removed, with non-ASCII characters removed, and tokenized.

\section{Prediction models}

% by Young
\subsection{Classification}
Through the hypothesis 5 and 6 in the article basically given for this project, Imai and Gelfand illustrate that counting \textit{sequences of integrative information behaviors} and \textit{sequences of cooperative relationship management behaviors} could be a clue for predicting \textit{joint profit}, higher number of counter would produce higher joint profit.
So, we can start the prediction with a classification task, predicting y = HIGH or LOW.  For this classification, we could use k-nearest neighbor algorithm. If the evaluation would be good we can classify dialogs into more levels of joint profit.\newline
Consequently, we are planning to implement an improved regression equation to predict joint profit of given test dialog. Imai and Gelfand included reciprocal, complementary, and cooperative sequences in the regression equation. We are now thinking about sparse linear regression equation for our prediction.

\subsection{Regression model}  % by Peng
The prediction of joint profit is a regression problem. We will compare the following regression models for this task:
\begin{enumerate}
\item Regression using SVM
\item Sparse linear regression
\item Weighted Nearest Neighbor + non-linear regression
\end{enumerate}

\section{Features}
\subsection{Feature Extraction}  % by Peng
We will test the predictive value of several features.

\textbf{Uni- and bigrams:} 

We can form a codebook of word or code (or unigrams) and a codebook of bigrams. Given a new negotiation dialogue, we can then compute two codeword distributions (or occurrences) and use them as feature vectors which will be later fed into a regression model.

-	We can compute the distributions for the two persons separately or jointly.

-	If we compute codeword distributions for two persons separately, we may take a look at how the two distributions are different from each other and compute, say, KL-divergence between them as a new feature.

\textbf{Demographic features} \\
mean age, negative experience, education, cultural intelligence, emotional intelligence, IQ, extroversion, openness

\textbf{Other}
Number of turns and length of dialogue: these two features would indicate whether the discussion is sufficient or not and the easiness of achieving agreement.

\subsection{Feature selection}  % by Peng
For feature selection, we may first divide dialogues into two classes, one with high joint profit and one with really low joint profit and find out the most discriminant unigrams and bigrams according to chi-square test and likelihood ratio test. These words will be used to form a codebook for computing the codebook based feature mentioned above.

\section{Evaluation and Baselines} % by Peng
\textbf{Evaluation of Classification Result}
Consider of the problem of classifying dialogues into three different classes, 1. LOW, 2. MEDIUM and 3. HIGH, we denote the number of samples whose true label is $i$ and predicted label is $j$ as $N_{ij}$, $i,j=1,2,3$. The performance of our algorithm is evaluated with three metrics: overall classification accuracy, precision and recall for each class. The three metrics are defined as follows:

\begin{equation*}
accuracy = \frac{N_{11}+N_{22}+N_{33}}{\sum_{i,j}N_{ij}}
\end{equation*}
\begin{equation*}
precision_{i}=\frac{N_{ii}}{N_{1i}+N_{2i}+N_{3i}}
\end{equation*}
\begin{equation*}
recall_{i}=\frac{N_{ii}}{N_{i1}+N_{i2}+N_{i3}}
\end{equation*}
where $precision_i$ and $recall_i$ are precision and recall for class i.

\textbf{Evaluation of Regression Result}
Three metrics can be used to evaluate the performance of the objective quality assessment model. The first metric is Spearman rank-order correlation coefficient between predicted joint profit and true joint Profit. It is related to prediction monotonicity of a model. The second metric is Pearson linear correlation coefficient between predicted joint profit and true joint Profit. It is considered as a measure of prediction accuracy of a model. The third metric is root mean squared error (rmse) between predicted joint profit and true joint Profit.

\end{document}
