% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...
\usepackage{amsmath,epsfig}
\usepackage{amsthm, amssymb}

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\usepackage{ textcomp }

%%% END Article customizations


%%% The "real" document content comes below...

\title{LING773 - Negotiations Project Report}
\author{Peng Ye, Youngil Kim, Olivia Buzek}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed

\begin{document}
\maketitle


\section{Introduction}
brief introduction
Who is in this group. Optionally, a rough breakdown of people's roles in the project,
\section{Preprocessing}

\section{Feature extraction}
In this section, we will introduce three types of features that are extracted for predicting joint-profit.
\subsection{Meta information}

\subsection{Code-based N-Gram feature}
In our data set, every thought unit is carefully coded by social scientist. The code of a thought unit is like a summary about this thought unit. It helps us to understand the functionality of a thought unit. Some code indicates positive interaction between speakers, for example, 'RP' means positive reaction; some code indicates negative interaction, for example, 'RN' means negative reaction. Intuitively, negative or positive interaction should have high correlation with joint profit, thus thought unit code is informative feature for predicting joint profit. The second type of feature we use is though unit code based feature. Consider a dyad as a sequence of code, we then extract code-based unigram and bigram as our featurs. Specifically, for each dyad we extract the following features:
\begin{itemize}
\item Unigram based features:
\begin{itemize}
\item Overall features: number of total thought units, number of thought units of each code, percentage of thought units of each code
\item Wine features: number of total thought units (from wine part), number of thought units of each code, percentage of thought units of each code
\item Grocery features: number of total thought units (from grocery part), number of thought units of each code, percentage of thought units of each annotation
\end{itemize}
\item Bigram based features:
\end{itemize}



\subsection{Word-based N-Gram feature}
We extract word-based n-grams through following process.
\begin{itemize}
\item Preprocessing with raw data - creating \textit{fields\_speaker.txt}: \newline
We analyzed \textit{merged\_turns.csv} and \textit{metadata.csv} for creating \textit{fields\_speaker.txt} file. Because some dialogues do not have \textit{joint profit} in \textit{metadata.csv} file, we should rule out those dialogues even though they exist in \textit{merged\_turns.csv} file. In addition, we rearranaged sequence of fields for better looking; the dyid is positioned as the first item.

\item Tokeninzing with whole dialog files: \newline
The raw data contains many special characters and not yet tokenized for creating n-grams. Basically, we used Treebank tokenization and this can be found in \textit{http://www.cis.upenn.edu/$\sim$treebank/tokenization.html}. Following list shows a part of the tokenization rules.
\begin{itemize}
\item most punctuation is split from adjoining words.
\item verb contractions and the Anglo-Saxon genitive of nouns are split into their component morphemes, and each morpheme is tagged separately.
\begin{itemize}
\item Examples
\begin{itemize}
\item children's \textrightarrow children 's
\item I'm \textrightarrow I 'm
\end{itemize}
\end{itemize}
This tokenization allows us to analyze each component separately.\newline
In addition, we need to correct some words to see more precise result. For example, some speakers omit \textit{question mark} or \textit{apostrophe}, and sometimes there are several special characters which is not interpretable in plain text. You can see the detailed conversion in \textit{tokenize} function in \textit{create\_ngram.py}.
\end{itemize}
\item Count n-grams and Create n-gram ID:\newline
After tokenizing, we count number of n-grams to see the characteristics of each dialogue. For counting ngrams, we used \textit{count.pl} in Ted Pedersen Ngram Statics Package with general stop words because stop words can hide real characteristics of n-grams. We seperately count n-grams for Wine, Grocery, and all dialogues for seeing the characteristics of each group of speakers. Also, we count n-grams per each dyad too. \newline
In addition, we created n-gram id files to express ngrams. For example, \textit{grand opening date} which is one of trigram is expressed as \textit{3-0001}. By using this ngram id, we can save memory and storage for analyzing whole data.
\item Make n-gram feature file:\newline
By using counted n-grams, we created n-gram feature file which can be converted into an ARFF format, input format of Weka. Each row in the file contains n-gram information of each dyad. The first column shows the \textit{dyID}, and the following colums show ngram id and the number of ngrams in the dialogues. Following example shows a part of the feature file.
\begin{itemize}
\item Example
\begin{verbatim}
02,1-0002:30,1-0001:23,1-0040:18,1-0009:16,1-0012:15 ... ...
03,1-0006:30,1-0001:29,1-0048:23,1-0026:21,1-0025:20 ... ...
\end{verbatim}
\end{itemize}
We made three n-gram feature files for each category, Wine, Grocery, and all dialogues.
\end{itemize}




\section{Regression}
\subsection{Feature Selection}
Feature subset selection is a process of identifying the most informative features and removing irrelevant and redundant features. In Weka, we use 'BestFirst' as search method and use 'CfsSubsetEval' as attribute evaluator for finding the most informative features. As is explained in Weka, 'CfsSubsetEval' evaluates ``the worth of a subset of attributes by considering the individual predictive ability of each feature along with the degree of redundancy between them'' and 'BestFirst' searches ``the space of attribute subsets by greedy hillclimbing augmented with a backtracking facility. Setting the number of consecutive non-improving nodes allowed controls the level of backtracking done.'' In our experiment, best first search start with the full set of attributes and search backward. The top ten best word based ngram feature is shown in Table.\ref{tab:selected_ngram} and the top ten best code based ngram feature is shown in Table.
\begin{table}
  \centering
  \caption{THE TOP TEN MOST INFORMATIVE WORD-BASED AND CODE-BASED NGRAM FEATURE}
  \begin{tabular}{|c|c|}
     \hline
CODE-BASED NGRAM FEATURE & WORD-BASED NGRAM \\
  \hline
percentage-of-(MIN-QR)-turns & hours \\

percentage-of-(SF)-overall & special\\

percentage-of-(SF)-grocery & great\\

number-of-(IR)-overall & 30k\\

percentage-of-(SBR-SF)-overall & agreeable\\

percentage-of-(OM-QM)-wine & 630\\

percentage-of-(MIN-IR)-wine & 1030pm\\

percentage-of-(IP-SF)-grocery & luxurious\\

percentage-of-(IP-IDN)-turn & people work\\

percentage-of-(IDN-SBR)-turns & crust grocery\\
  \hline
  \end{tabular}\label{tab:selected_ngram}
\end{table}
 
\subsection{Regression Algorithms}

\subsection{Evaluation Metrics}



\section{Experiment results}
\subsection{Baseline Approach}

\subsection{Improved Approach}

\end{document}
